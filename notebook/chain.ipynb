{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "from openai import OpenAI\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import Dict\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "from chatgpt_ocr import is_pdf_by_signature, encode_image, pdf_to_image, extract_json_from_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수에서 API 키 로드\n",
    "load_dotenv('../.env')\n",
    "openai.api_key = os.getenv(\"CHATGPT-RECEIPT\")\n",
    "client = OpenAI(api_key=os.getenv(\"CHATGPT-RECEIPT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR Chain 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRChain(Chain):\n",
    "    \"\"\"\n",
    "    OpenAI를 이용해서 OCR을 진행하는 사용자 정의 체인.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"image_path\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return [\"ocr_response\"]  # 출력값으로 Assistant의 응답을 반환\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        image_path = inputs[\"image_path\"]\n",
    "        # print(f\"Image path: {image_path}\")\n",
    "        try:\n",
    "            # 이미지 파일 열기\n",
    "            if is_pdf_by_signature(image_path):\n",
    "                base64_image = pdf_to_image(image_path)\n",
    "            else:\n",
    "                base64_image = encode_image(image_path)\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # GPT-4 Vision 모델 사용\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": \"You are an assistant that extracts information from receipt images.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": '''다음 텍스트는 영수증의 정보입니다. 이 텍스트에서 가게 이름, 날짜, 항목, 총액을 분석하고, 아래의 JSON형식으로 결과를 반환해 주세요.\n",
    "                                JSON 형식:\n",
    "                                {\n",
    "                                    \"상호명\": \"가게 이름\",\n",
    "                                    \"날짜\": \"YYYY-MM-DD\",\n",
    "                                    \"항목\": [\n",
    "                                        {\"이름\": \"상품1\", \"가격\": 상품1 가격},\n",
    "                                        {\"이름\": \"상품2\", \"가격\": 상품2 가격}\n",
    "                                    ],\n",
    "                                    \"총액\": 총액\n",
    "                                }\n",
    "                                반환할 JSON 형식은 반드시 위의 구조와 일치해야 하며, 불필요한 설명은 포함하지 마세요.\n",
    "                                영수증 텍스트:\n",
    "                                \"\"\"\n",
    "                                [여기에 영수증의 텍스트 또는 OCR로 추출한 내용이 들어갑니다]\n",
    "                                \"\"\"\n",
    "                                결과:''' \n",
    "                            },{\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                            }\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            # 응답 파싱\n",
    "            raw_response = extract_json_from_string(response.choices[0].message.content)\n",
    "            # print(f'raw_response = {raw_response}')\n",
    "            return {\"ocr_response\": raw_response}\n",
    "        except Exception as e:\n",
    "            \n",
    "            return {\"ocr_response\": f\"Error during OpenAI API call: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant Chain 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Chain 정의\n",
    "class OpenAIAssistantChain(Chain):\n",
    "    \"\"\"\n",
    "    OpenAI Assistant에 질의하는 사용자 정의 체인.\n",
    "    \"\"\"\n",
    "    def __init__(self, prompt: PromptTemplate):\n",
    "        super().__init__()\n",
    "        self._prompt = prompt\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        # PromptTemplate에서 정의된 입력 변수 사용\n",
    "        return self._prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return [\"assistant_response\"]\n",
    "    \n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        formatted_prompt = self._prompt.format(**inputs)\n",
    "        # OpenAI API 호출\n",
    "        thread_id = 'thread_Twa2ruyrfpBGhOqESAnAYn6W'\n",
    "        assistant_id = 'asst_7J3TrQfqCaD5dWXZpu7665l5'\n",
    "        message = client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content= formatted_prompt\n",
    "        )\n",
    "        # Run 생성 및 실행\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread_id,\n",
    "            assistant_id=assistant_id\n",
    "        )\n",
    "        # Run 완료 대기\n",
    "        while run.status != \"completed\":\n",
    "            run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)\n",
    "        # 결과 메시지 가져오기\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "        # print(f'messages = {messages}')\n",
    "        # 결과 출력\n",
    "        for i, message in enumerate(messages):\n",
    "            if message.role == \"assistant\":\n",
    "                if i == 0:\n",
    "                    response = message.content[0].text.value\n",
    "        # print(f'i = {i}, response = {response}')\n",
    "                \n",
    "        return {\"assistant_response\": response}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_prompt = PromptTemplate(\n",
    "    input_variables=[\"ocr_response\"],\n",
    "    template=\"\"\"다음은 분석 지침입니다:\n",
    "1. JSON 데이터에서 '상호명' 값을 확인하세요.\n",
    "2. 상호명을 검색하여 업종을 추론해 보세요.\n",
    "3. '상호명'을 기반으로 아래 카테고리 중 하나를 정확히 선택하세요:\n",
    "    - 회의비\n",
    "    - 교통비\n",
    "    - 식대\n",
    "4. 가능한 경우 단답형으로 항목만 답변하세요. (예: \"식대\")\n",
    "5. 정확한 카테고리를 판단할 수 없는 경우:\n",
    "    - \"판단 필요\"라고 답변하세요.\n",
    "    - \"판단 필요\"인 이유를 1~2 문장으로 간단히 설명하세요.\n",
    "6. 결과는 다음 형식으로 반환하세요:\n",
    "    - 카테고리: [선택된 항목 또는 판단 필요]\n",
    "    - 이유: [간단한 설명, 없을 경우 \"없음\"]\n",
    "\n",
    "JSON 데이터:\n",
    "{ocr_response}\n",
    "\n",
    "답변:\"\"\"\n",
    ")\n",
    "\n",
    "# OpenAI Assistant 체인\n",
    "assistant_chain = OpenAIAssistantChain(prompt=assistant_prompt)\n",
    "\n",
    "# OCR 체인\n",
    "ocr_chain = OCRChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "최종 결과:\n",
      "카테고리: 판단 필요\n",
      "이유: \"현대프리미엄아울렛 김포점 바버\"에서 구입한 '버버남성캐주얼'은 의류 구매를 나타내며, 제공된 카테고리(회의비, 교통비, 식대) 중 어디에도 명확하게 속하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"../image/IMG_9258.jpg\"  # 실제 이미지 파일 경로를 여기에 설정\n",
    "\n",
    "# SequentialChain 구성\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[ocr_chain, assistant_chain],\n",
    "    input_variables=[\"image_path\"],\n",
    "    output_variables=[\"assistant_response\"],\n",
    "    verbose=True\n",
    ")\n",
    "    \n",
    "# 실행\n",
    "result = sequential_chain.invoke({\"image_path\": image_path})\n",
    "print(\"\\n최종 결과:\")\n",
    "print(result['assistant_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최종 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# PromptTemplate 정의\n",
    "assistant_prompt = PromptTemplate(\n",
    "    input_variables=[\"ocr_response\"],\n",
    "    template=\"\"\"다음은 분석 지침입니다:\n",
    "1. JSON 데이터에서 '상호' 값을 확인하세요.\n",
    "2. 상호를 검색해서 업종을 판단해 주세요.\n",
    "3. '상호'를 기반으로 아래 카테고리 중 하나를 선택하세요:\n",
    "    - 회의비\n",
    "    - 교통비\n",
    "    - 식대\n",
    "4. 단답형으로 항목만 답변해 주세요. (ex, 출장비)\n",
    "5. 정확한 항목이 없을 경우, \"판단 필요\"라고 답변해 주세요.\n",
    "6. 판단이 필요한 경우에는 이유를 말해주세요.\n",
    "\n",
    "JSON 데이터:\n",
    "{ocr_response}\n",
    "답변:\"\"\"\n",
    ")\n",
    "\n",
    "# OCRChain 초기화\n",
    "ocr_chain = OCRChain()\n",
    "\n",
    "# OpenAI Assistant 체인 초기화\n",
    "assistant_chain = OpenAIAssistantChain(prompt=assistant_prompt)\n",
    "\n",
    "# SequentialChain 구성\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[ocr_chain, assistant_chain],\n",
    "    input_variables=[\"image_path\"],\n",
    "    output_variables=[\"assistant_response\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 테스트 이미지 경로 설정\n",
    "test_image_path = \"../image/hyundai.jpg\"\n",
    "\n",
    "# 체인 실행\n",
    "result = sequential_chain.invoke({\"image_path\": test_image_path})\n",
    "print(\"\\n최종 결과:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
